# Add Live Text interaction to your app
> [video](https://developer.apple.com/videos/play/wwdc2022/10026/)

## Talkshow
> 2023.05.10, 20:00 ~ 21:00 <br>
> 참여자: 강민규, 이다혜, 이봄이

<hr>

### Live Text 써보셨나요?
* `봄이💟`: 많이 쓰고 있다! 유용해유용해. 근데 이름은 모르고 쓰고 있었다.
* `다혜🐥`: 동감.
* `봄이💟`: 근데 이게 어떻게 구현되어 있는지, 앱에 적용할 수 있는지는 감도 안왔다. 잘 몰랐어요. 그리고 iOS16에서만 쓸 수 있네요? 다들 업데이트했나요?
* `다혜🐥`: 당근, 올해 초에 했습니다
* `민규🍀`: 저는 iOS16 나올 때부터 했습니다

* `다혜🐥`: 저는 맥에서 Live Text를 유용하게 썼습니다. 유튜브로 강의 볼 때 사진 캡쳐해서 ppt 내용 긁어서 썼어요!
* `봄이💟`: 어랏 굳이 캡쳐안해도 쓸 수 있어요
* `다혜🐥`: ???! 어떻게 해요
* `봄이💟`: WWDC 영상 볼 때 멈춰두고 텍스트 꾹 클릭하면 선택됩니다. 해보세여

(직접 확인해봄)

* `다혜🐥`: 이게 되네~ 유튜브에도 되나요?
* `봄이💟`: 어.. 확인 안해봤는데, 지금 확인 해보죠!

(직접 확인해봄)

* `다혜🐥`: 이것도 되네요?? 유튜브도 된다... 와 라이브 텍스트가 여기서도 먹히네요? 
* `봄이💟`: 어제 WWDC보면서 영상에도 적용된다길래, 바로 해봤는데 되더라구요 ㅎㅎ 편합니다!! 

<hr>

### 영상을 보면서 이해가 잘 안되던 부분?
* `봄이💟`: 영상 앞에서는 코드 적용과 흐름에 대해서 이야기했는데, 거기까지는 이해가 쉬웠어요. 근데 뒤로 갈수록 어려워졌습니다. 폰트조정, 위치조정까지는 좋았어요! 
근데 더더더 뒤로 갈수록 생소한 함수들이 나와서 이해가 어려웠어요. 
* `다혜🐥`: 동의. 특히 비전킷 부분에서는 생소한 메서드들이 많이 나왔어요. 사용해본적이 없어서 그런가 더 어려웠습니다. 
정적 이미지에서 텍스트 뽑는 건 앱에 적용하기 쉬울 것 같은데 멈춘 영상 프레임에서 라이브 텍스트를 적용하는 건,, 실제로 어떻게 쓸 수 있을지 감이 안왔다. 
* `봄이💟`: 영상보면 이걸로 이거 저거 다 만들어 보고 싶을 줄 알았는데, 생각보다 이 기술을 활용할 아이디어가 잘 안떠올랐다. 
* `민규🍀`, `다혜🐥`: 마잣. 크게 생각은 안났어.
* `민규🍀`: 근데 애플 안경이 나오면 생활에서 나오는 텍스트를 이런 라이브 텍스트로 번역해주지 않을까?
* `다혜🐥`: 오호. 안경 형태로 나오면 지금 영상에 나오는 정적 이미지가 아니라, 동적인 영상에서 텍스트를 뽑아내야겠네요. 
* `민규`: 이게 나오면 중국어 몰라도 중국으로 여행을 갈수도 있겠네요!
* `봄이💟`: 그건 지금도 할 수 있지않나요?

(일동 웃음)

* `봄이💟`: 진짜 그런 안경이 나온다면 외국에서 운전하기가 더 편해지겠죠? 검색하기 힘든 운전자에게 굉장히 편리할 것 같네요.
* `다혜🐥`: 저희 3명 다 영상 뒤로 갈수록 어려웠나보네요. 그래도 많이 쓰는 기술이라 이것저것 많이 이야기가 나오는 것 같아요.
* `봄이💟`: 처음엔 어? 쉽네? 했는데 뒤로 갈수록 난이도가 높아졌어요. 그래도 재밌었습니다~

<hr>

### 그럼 이 기술을 어디에 적용해볼 수 있을까?
* `다혜🐥`: 다음 이야기 주제로 토론해보려고 했던건데, 이미 앞에서 말이 나왔네요! 하하 그럼 살~짝 방향을 틀어서, 지금 이 기술이 각자 만들고 있는 프로젝트에 적용해볼 수있을까요?
* `봄이💟`: 제가 하고 있는 프로젝트는 'AI 치매 예방 및 자가진단'을 주제로 앱을 만들고 있는데, 여기서 사진을 이용한 기능은 없어서 바로 적용하기는 힘들 것 같아요. 
근데 생각해보면 있을 것 같기도? 다혜님은 있으신가요?
* `다혜🐥`: 저는~~ 흠 크게 생각나지는 않는데 제가 만든 '가사오케' 앱에서 QR을 이용하면 좀 재미있을지도? 
내 보관함에 있는 노래 목록을 QR코드로 만들고, 이걸 친구한테 보여주고, 카메라로 찍으면 그 친구의 가사오케에 제 보관함이 copy되는거죠. 급하게 생각난거라 좀 러프합니다. 
* `민규🍀`: 저도 크게 생각나지 않네요. 새로 프로젝트를 만든다면 모를까? 기존 프로젝트에서는 어려울 것 같습니다. 
(생각중) 아! 영수증과 관련된 앱을 만든다면, 쉽게 적용가능할 것 같아요! 영수증을 찍어서 Live text로 글을 뽑아내는 거죠~
* `봄이💟`: Live text로 글을 뽑는 것 까지 가능할까요? 그러니까, 텍스트가 인식은 되는데 그건 사용자가 액션을 직접 취해야 텍스트를 가져올 수 있는거 아닌가 해서요. 
따로 추출이 되는지는 영상에서 확인을 못해봤네요. 글을 인식해서 사용자에게 어떤 액션을 취할래? 정도만 해주는 것 같아서요.
* `민규🍀`: 오 그 부분은 확인이 필요해보이네요. 영상에서는 자세히 안다룬 부분이네요. 좋은 지적입니당

<hr>

### 마지막 질문, 위에 나왔던 질문 이외에 말하고 싶은게 있나요?
* `다혜🐥`: 이제 마지막입니다! 위에 나온 질문말고 이야기하고 싶은게 있나요? 소감도 좋습니다.
* `봄이💟`: 생각보다 깊은 내부 구조는 안알려주고 사용방법 위주로 알려줘서 쉽게 볼 수 있었어요. 
그래서 겉햝기 느낌이 강하게 들지만, 깊이 공부를 해보고 싶다는 생각은 안들었어요. 
내 앱에 적용해봐야겠다! 라는 생각이 들었으면 깊게 팠을텐데 그런 점이 크게 없어서 가볍게 시청했습니다. 
그래도 잘 사용하는 기능이라서 '이렇게 동작하는구나' 정도로 봤어요. 재밌었습니다.
* `다혜🐥`: 일단 재밌었습니다. 저도 자주 쓰는 기능이라 더 흥미로웠는데, 한번 적용해보고 싶은 생각이 들었어요. 
적어도 데모 프로젝트는 만들어보고 싶다? 기존 프로젝트에 적용은 못해도 한번 만져보고 싶은 API였어요!
* `민규🍀`: 토크쇼 재밌는데 많이 참가해주셨으면 좋겠네요.

* `다혜🐥`: 오늘 첫 토크쇼였는데, 즐겁게 진행한 것 같습니다. 편안한 분위기에서 이야기하니까 좋네요! Live Text를 쓸 때마다 이 영상이 생각날 것 같아요. 
다음 영상은 "Build global apps: Localization by example"입니다. 2주차에도 또 봤으면 좋겠습니다. 안녕!

<hr>

<br>

## 개인 정리본
| 이름 | 링크, 블로그 |
|:---:|:--:|
| 이다혜 | [[WWDC22] iOS16부터 Live Text를 사용할 수 있다! 직접 앱에 적용해보자.](https://github.com/Charming-Swift/WWDC-talkshow/blob/main/WWDC22/Add%20Live%20Text%20interaction%20to%20your%20app/archive/dahae0320.md) |
| 이봄이 | [Add Live Text interaction to your app 정리](https://github.com/Charming-Swift/WWDC-talkshow/blob/main/WWDC22/Add%20Live%20Text%20interaction%20to%20your%20app/archive/Add%20Live%20Text%20interaction%20to%20your%20app_note.md) |
| - | - |
